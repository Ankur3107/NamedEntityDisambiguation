{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "import pickle\n",
    "import sqlite3\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do data manipulation in SQL instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # data of both wikipedia pages \n",
    "    sample = pd.read_csv('../data/plaintext_link_sample.csv')\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    \n",
    "    # connect to database\n",
    "    db = sqlite3.connect('../data/kensho.db')\n",
    "    c = db.cursor()\n",
    "\n",
    "    # visualize query into pandas dataframe\n",
    "    def viz_tables(cols, query):\n",
    "        q = c.execute(query).fetchall()\n",
    "        framelist = dict()\n",
    "        for i, col_name in enumerate(cols):\n",
    "            framelist[col_name] = [col[i] for col in q]\n",
    "        return pd.DataFrame.from_dict(framelist)\n",
    "    \n",
    "    # column names\n",
    "    plaintext_link_cols = [col[1] for col in c.execute(\"PRAGMA table_info(plaintext_link)\")]\n",
    "    \n",
    "    # get a sample of the data to work on pandas\n",
    "    query2 = \"\"\"\n",
    "    SELECT *\n",
    "    FROM plaintext_link \n",
    "    WHERE source_section_id IN (SELECT DISTINCT(source_section_id) \n",
    "                                FROM plaintext_link \n",
    "                                ORDER BY random() \n",
    "                                LIMIT 5000)\n",
    "    \"\"\"\n",
    "    sample = viz_tables(plaintext_link_cols, query2)\n",
    "    sample.to_csv('../data/plaintext_link_sample.csv', index=False)\n",
    "    \n",
    "    c.close()\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # update database\n",
    "# # note we only save the introduction of all the wikipedia articles for now due to memory limitations\n",
    "# query = \"\"\"\n",
    "# CREATE TABLE plaintext_link AS\n",
    "# WITH link_intro AS (\n",
    "#     SELECT \n",
    "#         l.link_id, l.source_section_id, l.source_wikidata_numeric_id, l.link_anchor, \n",
    "#         l.link_offset_start, l.link_offset_end, l.target_wikidata_numeric_id\n",
    "#     FROM link l\n",
    "#     WHERE l.source_section_num = 0\n",
    "# )  \n",
    "# SELECT \n",
    "#     l.link_id, l.source_section_id, l.source_wikidata_numeric_id, l.link_anchor, \n",
    "#     l.link_offset_start, l.link_offset_end, l.target_wikidata_numeric_id, \n",
    "#     p.section_text, p.section_len, \n",
    "#     r.anchor_target_count, r.anchor_frac, r.target_frac\n",
    "# FROM link_intro l LEFT JOIN plaintext p\n",
    "# ON p.section_id = l.source_section_id\n",
    "# LEFT JOIN raw_anchor r\n",
    "# ON l.link_anchor = r.anchor_text\n",
    "# AND l.target_wikidata_numeric_id = r.target_wikidata_numeric_id\n",
    "# \"\"\"\n",
    "# c.execute(query)\n",
    "# db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data of wikidata entries\n",
    "wikidata = pd.read_csv('../data/wikipages_cleaned.csv')\n",
    "wikidata.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace words within paranthesis and remove underscores\n",
    "# get_rid of `the` which can cause difference\n",
    "pattern = re.compile(r'\\([^)]*\\)|_')\n",
    "clean_text = lambda text: re.sub(r'\\([^)]*\\)', \"\", text).replace('_', ' ').strip()\n",
    "wikidata.loc[:,'target_page_title'] = wikidata.apply(lambda i: clean_text(i['target_page_title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to replace lnk anchors with actual wikidata page titles\n",
    "def replace_anchor_text(text, replace_text, replace_start, replace_end):\n",
    "    \"\"\"\n",
    "    Given a text, a given phrase/word to replace into the text, and a given start and end position\n",
    "    of the original text to replace, return a list consisting of the new text, and the new start and end\n",
    "    indexes corresponding to the replaced phrase/word, and an offset to shift the characters of the text\n",
    "    \"\"\"\n",
    "    import string\n",
    "    new_replace_end = replace_start+len(replace_text)\n",
    "    # replace full words\n",
    "    if len(text) > replace_end:\n",
    "        while text[replace_end] in string.ascii_letters:\n",
    "            replace_end+=1\n",
    "    offset = new_replace_end - replace_end\n",
    "    new_text = text[:replace_start] + replace_text + text[replace_end:]\n",
    "    return [new_text, replace_start, new_replace_end, offset]\n",
    "\n",
    "# get corresponding wikidata page titles\n",
    "\n",
    "merged_sample = sample.merge(wikidata[['wikidata_numeric_id', 'target_page_title']], \n",
    "                               'left', left_on='target_wikidata_numeric_id', right_on='wikidata_numeric_id')\n",
    "# SOMEHOW WHEN IT MERGES, IT DUPLICATES ALOT OF ROWS\n",
    "merged_sample.drop_duplicates(inplace=True)\n",
    "\n",
    "replaced_sample = []\n",
    "# replace link anchors with actual wikidata page titles\n",
    "# extract replaced text and indexes\n",
    "for sid in merged_sample['source_section_id'].unique():\n",
    "    replace_section = merged_sample[merged_sample['source_section_id'] == sid].sort_values('link_offset_start').reset_index(drop=True)\n",
    "    text = replace_section['section_text'].loc[0]\n",
    "    offset = 0\n",
    "    for i in np.arange(replace_section.shape[0]):\n",
    "        replace_ls = replace_anchor_text(text, replace_section.loc[i,'target_page_title'], \n",
    "                                         replace_section.loc[i,'link_offset_start']+offset,\n",
    "                                         replace_section.loc[i,'link_offset_end']+offset)\n",
    "        text = replace_ls[0]\n",
    "        replace_section.loc[i, 'link_offset_start'] = replace_ls[1]\n",
    "        replace_section.loc[i, 'link_offset_end'] = replace_ls[2]\n",
    "#     replace_section.loc[i, 'link_anchor'] = replace_section.loc[i,'target_page_title']\n",
    "        offset += replace_ls[3]\n",
    "    replace_section['section_text'] = text\n",
    "    replaced_sample.append(replace_section)\n",
    "    \n",
    "# get back dataframe\n",
    "replaced_sample = pd.concat(replaced_sample)\n",
    "# edit section length and link anchors\n",
    "replaced_sample.loc[:,'link_anchor'] = replaced_sample.loc[:,'target_page_title']\n",
    "replaced_sample.loc[:,'section_len'] = replaced_sample.apply(lambda i: len(i['section_text']), axis=1)\n",
    "\n",
    "# drop redundant columns\n",
    "replaced_sample.drop(['wikidata_numeric_id', 'target_page_title'], axis=1, inplace=True)\n",
    "\n",
    "# check whether link anchor corresponds to position in text\n",
    "check_anchor = lambda row_data: row_data['section_text'][row_data['link_offset_start']:row_data['link_offset_end']] == row_data['link_anchor']\n",
    "assert all(replaced_sample.apply(check_anchor, axis=1))\n",
    "assert len(replaced_sample['section_text'].unique())==5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>source_section_id</th>\n",
       "      <th>source_wikidata_numeric_id</th>\n",
       "      <th>link_anchor</th>\n",
       "      <th>link_offset_start</th>\n",
       "      <th>link_offset_end</th>\n",
       "      <th>target_wikidata_numeric_id</th>\n",
       "      <th>section_text</th>\n",
       "      <th>section_len</th>\n",
       "      <th>anchor_target_count</th>\n",
       "      <th>anchor_frac</th>\n",
       "      <th>target_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24000001404979</td>\n",
       "      <td>24000000296809</td>\n",
       "      <td>6662070.0</td>\n",
       "      <td>Order of the British Empire</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>14420</td>\n",
       "      <td>Llewellyn Heycock, Baron Heycock Order of the ...</td>\n",
       "      <td>981</td>\n",
       "      <td>5283.0</td>\n",
       "      <td>0.994728</td>\n",
       "      <td>0.181478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24000001404980</td>\n",
       "      <td>24000000296809</td>\n",
       "      <td>6662070.0</td>\n",
       "      <td>Wales</td>\n",
       "      <td>106</td>\n",
       "      <td>111</td>\n",
       "      <td>25</td>\n",
       "      <td>Llewellyn Heycock, Baron Heycock Order of the ...</td>\n",
       "      <td>981</td>\n",
       "      <td>3620.0</td>\n",
       "      <td>0.351866</td>\n",
       "      <td>0.174231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24000001404981</td>\n",
       "      <td>24000000296809</td>\n",
       "      <td>6662070.0</td>\n",
       "      <td>Life peer</td>\n",
       "      <td>143</td>\n",
       "      <td>152</td>\n",
       "      <td>2914468</td>\n",
       "      <td>Llewellyn Heycock, Baron Heycock Order of the ...</td>\n",
       "      <td>981</td>\n",
       "      <td>1613.0</td>\n",
       "      <td>0.995679</td>\n",
       "      <td>0.729534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24000001404982</td>\n",
       "      <td>24000000296809</td>\n",
       "      <td>6662070.0</td>\n",
       "      <td>Margam</td>\n",
       "      <td>182</td>\n",
       "      <td>188</td>\n",
       "      <td>6759079</td>\n",
       "      <td>Llewellyn Heycock, Baron Heycock Order of the ...</td>\n",
       "      <td>981</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.971963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24000001404983</td>\n",
       "      <td>24000000296809</td>\n",
       "      <td>6662070.0</td>\n",
       "      <td>Great Western Railway</td>\n",
       "      <td>239</td>\n",
       "      <td>260</td>\n",
       "      <td>843251</td>\n",
       "      <td>Llewellyn Heycock, Baron Heycock Order of the ...</td>\n",
       "      <td>981</td>\n",
       "      <td>3320.0</td>\n",
       "      <td>0.812531</td>\n",
       "      <td>0.902419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          link_id  source_section_id  source_wikidata_numeric_id  \\\n",
       "0  24000001404979     24000000296809                   6662070.0   \n",
       "1  24000001404980     24000000296809                   6662070.0   \n",
       "2  24000001404981     24000000296809                   6662070.0   \n",
       "3  24000001404982     24000000296809                   6662070.0   \n",
       "4  24000001404983     24000000296809                   6662070.0   \n",
       "\n",
       "                   link_anchor  link_offset_start  link_offset_end  \\\n",
       "0  Order of the British Empire                 33               60   \n",
       "1                        Wales                106              111   \n",
       "2                    Life peer                143              152   \n",
       "3                       Margam                182              188   \n",
       "4        Great Western Railway                239              260   \n",
       "\n",
       "   target_wikidata_numeric_id  \\\n",
       "0                       14420   \n",
       "1                          25   \n",
       "2                     2914468   \n",
       "3                     6759079   \n",
       "4                      843251   \n",
       "\n",
       "                                        section_text  section_len  \\\n",
       "0  Llewellyn Heycock, Baron Heycock Order of the ...          981   \n",
       "1  Llewellyn Heycock, Baron Heycock Order of the ...          981   \n",
       "2  Llewellyn Heycock, Baron Heycock Order of the ...          981   \n",
       "3  Llewellyn Heycock, Baron Heycock Order of the ...          981   \n",
       "4  Llewellyn Heycock, Baron Heycock Order of the ...          981   \n",
       "\n",
       "   anchor_target_count  anchor_frac  target_frac  \n",
       "0               5283.0     0.994728     0.181478  \n",
       "1               3620.0     0.351866     0.174231  \n",
       "2               1613.0     0.995679     0.729534  \n",
       "3                104.0     0.832000     0.971963  \n",
       "4               3320.0     0.812531     0.902419  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ground_truth_helper(section):\n",
    "    \"\"\"\n",
    "    Helper function for extract_ground_truth\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import spacy\n",
    "    # tokenize text and extract corresponding tokens for anchor text\n",
    "    tokenizer = spacy.load('en_core_web_sm')\n",
    "    tokenized_text = tokenizer(section.section_text)\n",
    "    # base vector\n",
    "    tokenized_vector = np.zeros(len(tokenized_text))\n",
    "    # get span object to match characters with tokens\n",
    "    char_to_token = tokenized_text.char_span(section.link_offset_start, section.link_offset_end)\n",
    "    # label corresponding tokens if there is an anchor link\n",
    "    # to match anchor text rather than anchor link\n",
    "    if not char_to_token:\n",
    "        char_tokens = np.array([token.idx for token in tokenized_text])\n",
    "        closest_start_token = char_tokens[np.where(char_tokens <= section.link_offset_start)[0][-1]]\n",
    "        # to account for tokens at end of text\n",
    "        if np.where(char_tokens >= section.link_offset_end)[0].size > 0:\n",
    "            closest_end_token = char_tokens[np.where(char_tokens >= section.link_offset_end)[0][0]]-1\n",
    "        else:\n",
    "            closest_end_token = char_tokens[-1]-1\n",
    "        char_to_token = tokenized_text.char_span(closest_start_token, closest_end_token)\n",
    "        # weird quirk of spacy, probably some bug\n",
    "        # this bug makes it impossible to identify the last tokens if it is an entity\n",
    "        # as such, for entities which appear in the 2nd last token, perhaps we can view the last token as an entity\n",
    "        if not char_to_token:\n",
    "            char_to_token = tokenized_text.char_span(closest_start_token, closest_end_token+1)\n",
    "    tokenized_vector[char_to_token.start:char_to_token.end] = 1\n",
    "\n",
    "    # extract other relevant information\n",
    "    data = {(section.link_anchor, section.link_offset_start, \n",
    "             section.link_offset_end, \n",
    "             section.target_wikidata_numeric_id): tokenized_vector}\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extract_ground_truth(data):\n",
    "    \"\"\"\n",
    "    Given a data of wikipedia articles, extract in the format of a list in this format\n",
    "    and extract the true entities (wikidata links), start characters, end characters, and true Wikidata entry IDs \n",
    "    as the key of a dictionary with a vector of zeros, with ones at the positions of the\n",
    "    corresponding token positions of the true entities\n",
    "    i.e. ['Apple is a good company', {('Apple', 0, 6, 0101102086): [1,0,0,0,0]}]\n",
    "    \"\"\" \n",
    "    sample_truth = []\n",
    "    \n",
    "    # for each section of text\n",
    "    for sid in data['source_section_id'].unique():\n",
    "        text = data.loc[data['source_section_id']==sid, 'section_text'].iloc[0]\n",
    "    \n",
    "        # extract relevant data from dataframe \n",
    "        entity_data = (data[data['source_section_id']==sid]\n",
    "                       .apply(extract_ground_truth_helper, axis=1))\n",
    "              \n",
    "        # form the correct data structure \n",
    "        sample_truth.append([text, dict()])\n",
    "        for entity in list(entity_data):\n",
    "            sample_truth[-1][-1].update(entity)\n",
    "    return sample_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabel = extract_ground_truth(replaced_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_or_not(arr):\n",
    "    \"\"\"\n",
    "    Given an array of 'ENT_IOB' outputs from a SpaCy tokenizer, where \n",
    "    3: token begins entity, 2: outside entity, 1: inside entity, 0: no entity tag\n",
    "    converts it to 1's and 0's where 1: entity, 0: no entity\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    return np.array([1 if i in [1,3] else 0 for i in arr])\n",
    "\n",
    "def extract_nlp_data(text):\n",
    "    \"\"\"\n",
    "    Given a text string, run SpaCy NER on the text string,\n",
    "    and extract the identified entities, start characters, end characters, and the entities labels\n",
    "    correspondingly as the key of a dictionary with a vector of zeros, with ones at the positions of the\n",
    "    corresponding token positions of the identified entities\n",
    "    i.e. ['Apple is a good company', {('Apple', 0, 6, 'ORG'): [1,0,0,0,0]}]\n",
    "    \"\"\"\n",
    "    import spacy\n",
    "    spacy_tagger = spacy.load('en_core_web_sm')\n",
    "    spacy_res = spacy_tagger(text)\n",
    "    \n",
    "    ner_res = {}\n",
    "    for entity in spacy_res.ents:\n",
    "        if entity.label_ in ['DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']:\n",
    "            continue\n",
    "        tokenized_vector = np.zeros(len(spacy_res))\n",
    "        char_to_token = spacy_res.char_span(entity.start_char, entity.end_char)\n",
    "        tokenized_vector[char_to_token.start:char_to_token.end] = 1        \n",
    "#         tokenized_vector = entity_or_not(spacy_res.to_array('ENT_IOB'))\n",
    "        ner_res[(entity.text, entity.start_char, entity.end_char, entity.label_)] = tokenized_vector\n",
    "    \n",
    "    return [text, ner_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all text data, do NER and extract all useful data\n",
    "sample_ner = [extract_nlp_data(text) for text in replaced_sample['section_text'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_no_entities(ner_data, label_data):\n",
    "    \"\"\"\n",
    "    Given NER data and true labels data, both in the format\n",
    "    i.e. [['Apple is a good company', {('Apple', 0, 6, 0101102086): [1,0,0,0,0]}],...]\n",
    "    check if there are text documents with either no entities identified by NER, or no\n",
    "    entities with Wikidata links. Drop all of these text documents, and returns the NER \n",
    "    and true labels data.\n",
    "    \"\"\"\n",
    "    # check to drop all texts without any ner_entity, or any true entity\n",
    "    ner_entities_missing = [i for i, text in enumerate(ner_data) if not text[1]]\n",
    "    true_entities_missing = [i for i, text in enumerate(label_data) if not text[1]]\n",
    "    \n",
    "    # drop entries if either does not have any entities\n",
    "    new_ner_data = [text for i, text in enumerate(ner_data) if i not in ner_entities_missing+true_entities_missing]\n",
    "    new_label_data = [text for i, text in enumerate(label_data) if i not in ner_entities_missing+true_entities_missing]\n",
    "    return new_ner_data, new_label_data\n",
    "\n",
    "sample_ner, ylabel = drop_no_entities(sample_ner, ylabel)\n",
    "\n",
    "with open('../data/sample_full_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(ylabel, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../data/sample_data.pkl', 'wb') as f:\n",
    "    pickle.dump(sample_ner, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now extract the NER entities that also have a corresponding 'true' Wikidata link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_01_vector(data, axis):\n",
    "    \"\"\"\n",
    "    Given data in the format \n",
    "    i.e. [['Apple is a good company', {('Apple', 0, 6, 0101102086): [1,0,0,0,0]}],...]\n",
    "    extract all the 0-1 vectors within each dictionary, \n",
    "    concatenate them together along a given axis, and repeat for every single entry in the data, \n",
    "    where each entry in the data corresponds to a text document. \n",
    "    The function then returns 2 items, the first a list of arrays corresponding to the concatenated\n",
    "    0-1 vectors, the second a list (of the same length) of dictionaries where the key, value pairs \n",
    "    correspond to the index within the arrays (from the 1st item) and the matching key pair of entity\n",
    "    matches from the data respectively\n",
    "    \"\"\"\n",
    "    key_idx_dict_ls = []\n",
    "    concat_vec_all = []\n",
    "    for text in data:\n",
    "        key_idx_dict = {}\n",
    "        concat_vec = []\n",
    "        # to check if there are entities in the data\n",
    "        if not text[1]:\n",
    "            raise Assertion('Drop text documents without entities!')\n",
    "        else:\n",
    "            for i, (key, val) in enumerate(text[1].items()):\n",
    "                concat_vec.append(val)\n",
    "                key_idx_dict[i] = key\n",
    "            concat_vec = np.stack(concat_vec, axis=axis)\n",
    "\n",
    "            # each entry in the list corresponds to a text\n",
    "            concat_vec_all.append(concat_vec)\n",
    "            key_idx_dict_ls.append(key_idx_dict)\n",
    "    return concat_vec_all, key_idx_dict_ls\n",
    "\n",
    "# for each identified NER entity in each text\n",
    "ner_entities, _ = extract_01_vector(sample_ner, axis=1)\n",
    "\n",
    "# possible true entities with wikidata links\n",
    "true_entities, key_idx_dict_ls = extract_01_vector(ylabel, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of True Entities identified by NER: 0.8430122402333764\n"
     ]
    }
   ],
   "source": [
    "# for each true entity in each text, check for a corresponding identified NER entity\n",
    "# if there is no corresponding identified NER entity, drop the true entity\n",
    "# check for the percentage of entities correctly identified by NER\n",
    "n_true_ent = 0\n",
    "n_ner_ent = 0\n",
    "entity_check_idx_ls = []\n",
    "for i in np.arange(len(true_entities)):\n",
    "    # FOR NOW, AS LONG AS THERE IS SOME OVERLAP, WE KEEP THE TRUE ENTITY\n",
    "    entity_check = np.sum(true_entities[i] @ ner_entities[i], axis=1)\n",
    "    \n",
    "    # entities which have a Wikidata link, but do not have a corresponding NER entity\n",
    "    entity_check_idx = np.where(entity_check==0)[0]\n",
    "    entity_check_idx_ls.append(entity_check_idx)\n",
    "    \n",
    "    # percentage of entities identified\n",
    "    n_true_ent += entity_check.shape[0]\n",
    "    n_ner_ent += entity_check.shape[0]-len(entity_check_idx)\n",
    "\n",
    "\n",
    "print('Percentage of True Entities identified by NER: {}'.format(n_ner_ent/n_true_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all those not identified entities in the true entities list\n",
    "for data, idx_ls, key_idx in zip(ylabel, entity_check_idx_ls, key_idx_dict_ls):\n",
    "    for i in idx_ls:\n",
    "        del data[1][key_idx[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sample_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(ylabel, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
