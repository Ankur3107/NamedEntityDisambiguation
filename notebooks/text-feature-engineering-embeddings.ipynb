{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "import pickle\n",
    "import sqlite3\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do data manipulation in SQL instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # data of both wikipedia pages \n",
    "    sample = pd.read_csv('../data/plaintext_link_sample.csv')\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    \n",
    "    # connect to database\n",
    "    db = sqlite3.connect('../data/kensho.db')\n",
    "    c = db.cursor()\n",
    "\n",
    "    # visualize query into pandas dataframe\n",
    "    def viz_tables(cols, query):\n",
    "        q = c.execute(query).fetchall()\n",
    "        framelist = dict()\n",
    "        for i, col_name in enumerate(cols):\n",
    "            framelist[col_name] = [col[i] for col in q]\n",
    "        return pd.DataFrame.from_dict(framelist)\n",
    "    \n",
    "    # column names\n",
    "    plaintext_link_cols = [col[1] for col in c.execute(\"PRAGMA table_info(plaintext_link)\")]\n",
    "    \n",
    "    # get a sample of the data to work on pandas\n",
    "    query2 = \"\"\"\n",
    "    SELECT *\n",
    "    FROM plaintext_link \n",
    "    WHERE source_section_id IN (SELECT DISTINCT(source_section_id) \n",
    "                                FROM plaintext_link \n",
    "                                ORDER BY random() \n",
    "                                LIMIT 5000)\n",
    "    \"\"\"\n",
    "    sample = viz_tables(plaintext_link_cols, query2)\n",
    "    sample.to_csv('../data/plaintext_link_sample.csv', index=False)\n",
    "    \n",
    "    c.close()\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # update database\n",
    "# # note we only save the introduction of all the wikipedia articles for now due to memory limitations\n",
    "# query = \"\"\"\n",
    "# CREATE TABLE plaintext_link AS\n",
    "# WITH link_intro AS (\n",
    "#     SELECT \n",
    "#         l.link_id, l.source_section_id, l.source_wikidata_numeric_id, l.link_anchor, \n",
    "#         l.link_offset_start, l.link_offset_end, l.target_wikidata_numeric_id\n",
    "#     FROM link l\n",
    "#     WHERE l.source_section_num = 0\n",
    "# )  \n",
    "# SELECT \n",
    "#     l.link_id, l.source_section_id, l.source_wikidata_numeric_id, l.link_anchor, \n",
    "#     l.link_offset_start, l.link_offset_end, l.target_wikidata_numeric_id, \n",
    "#     p.section_text, p.section_len, \n",
    "#     r.anchor_target_count, r.anchor_frac, r.target_frac\n",
    "# FROM link_intro l LEFT JOIN plaintext p\n",
    "# ON p.section_id = l.source_section_id\n",
    "# LEFT JOIN raw_anchor r\n",
    "# ON l.link_anchor = r.anchor_text\n",
    "# AND l.target_wikidata_numeric_id = r.target_wikidata_numeric_id\n",
    "# \"\"\"\n",
    "# c.execute(query)\n",
    "# db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unicode_dict():\n",
    "    import sys\n",
    "    import unicodedata\n",
    "    \n",
    "    # get all unicode accented characters for alphabets\n",
    "    unicode_dict = {}\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "        letter_str = '['\n",
    "        if letter.isupper():\n",
    "            caps = 'CAPITAL '\n",
    "        else:\n",
    "            caps = 'SMALL '\n",
    "        for i in range(sys.maxunicode):\n",
    "            try:\n",
    "                if caps+'LETTER '+letter.upper()+' ' in unicodedata.name(chr(i)):\n",
    "                    letter_str+=chr(i)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        unicode_dict[letter] = letter_str+']'\n",
    "    return unicode_dict\n",
    "\n",
    "unicode_dict = get_unicode_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>source_section_id</th>\n",
       "      <th>source_wikidata_numeric_id</th>\n",
       "      <th>link_anchor</th>\n",
       "      <th>link_offset_start</th>\n",
       "      <th>link_offset_end</th>\n",
       "      <th>target_wikidata_numeric_id</th>\n",
       "      <th>section_text</th>\n",
       "      <th>section_len</th>\n",
       "      <th>anchor_target_count</th>\n",
       "      <th>anchor_frac</th>\n",
       "      <th>target_frac</th>\n",
       "      <th>check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29759</th>\n",
       "      <td>37496</td>\n",
       "      <td>1663</td>\n",
       "      <td>335806.0</td>\n",
       "      <td>Ge'ez</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>35667</td>\n",
       "      <td>An abugida from Geez abugida or alphasyllabary...</td>\n",
       "      <td>2308</td>\n",
       "      <td>312.0</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.601156</td>\n",
       "      <td>Geez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>37497</td>\n",
       "      <td>1663</td>\n",
       "      <td>335806.0</td>\n",
       "      <td>consonant</td>\n",
       "      <td>142</td>\n",
       "      <td>151</td>\n",
       "      <td>38035</td>\n",
       "      <td>An abugida from Geez abugida or alphasyllabary...</td>\n",
       "      <td>2308</td>\n",
       "      <td>943.0</td>\n",
       "      <td>0.946787</td>\n",
       "      <td>0.827193</td>\n",
       "      <td>consonant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>37498</td>\n",
       "      <td>1663</td>\n",
       "      <td>335806.0</td>\n",
       "      <td>vowel</td>\n",
       "      <td>163</td>\n",
       "      <td>168</td>\n",
       "      <td>36244</td>\n",
       "      <td>An abugida from Geez abugida or alphasyllabary...</td>\n",
       "      <td>2308</td>\n",
       "      <td>998.0</td>\n",
       "      <td>0.993035</td>\n",
       "      <td>0.740356</td>\n",
       "      <td>vowel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26768</th>\n",
       "      <td>37499</td>\n",
       "      <td>1663</td>\n",
       "      <td>335806.0</td>\n",
       "      <td>alphabet</td>\n",
       "      <td>218</td>\n",
       "      <td>226</td>\n",
       "      <td>9779</td>\n",
       "      <td>An abugida from Geez abugida or alphasyllabary...</td>\n",
       "      <td>2308</td>\n",
       "      <td>625.0</td>\n",
       "      <td>0.795165</td>\n",
       "      <td>0.823452</td>\n",
       "      <td>alphabet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15274</th>\n",
       "      <td>37500</td>\n",
       "      <td>1663</td>\n",
       "      <td>335806.0</td>\n",
       "      <td>abjad</td>\n",
       "      <td>287</td>\n",
       "      <td>292</td>\n",
       "      <td>185087</td>\n",
       "      <td>An abugida from Geez abugida or alphasyllabary...</td>\n",
       "      <td>2308</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.985075</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>abjad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       link_id  source_section_id  source_wikidata_numeric_id link_anchor  \\\n",
       "29759    37496               1663                    335806.0       Ge'ez   \n",
       "1590     37497               1663                    335806.0   consonant   \n",
       "1909     37498               1663                    335806.0       vowel   \n",
       "26768    37499               1663                    335806.0    alphabet   \n",
       "15274    37500               1663                    335806.0       abjad   \n",
       "\n",
       "       link_offset_start  link_offset_end  target_wikidata_numeric_id  \\\n",
       "29759                 16               20                       35667   \n",
       "1590                 142              151                       38035   \n",
       "1909                 163              168                       36244   \n",
       "26768                218              226                        9779   \n",
       "15274                287              292                      185087   \n",
       "\n",
       "                                            section_text  section_len  \\\n",
       "29759  An abugida from Geez abugida or alphasyllabary...         2308   \n",
       "1590   An abugida from Geez abugida or alphasyllabary...         2308   \n",
       "1909   An abugida from Geez abugida or alphasyllabary...         2308   \n",
       "26768  An abugida from Geez abugida or alphasyllabary...         2308   \n",
       "15274  An abugida from Geez abugida or alphasyllabary...         2308   \n",
       "\n",
       "       anchor_target_count  anchor_frac  target_frac      check  \n",
       "29759                312.0     0.753623     0.601156       Geez  \n",
       "1590                 943.0     0.946787     0.827193  consonant  \n",
       "1909                 998.0     0.993035     0.740356      vowel  \n",
       "26768                625.0     0.795165     0.823452   alphabet  \n",
       "15274                132.0     0.985075     0.660000      abjad  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correct_whitespace_offset(data):\n",
    "    data = data.copy()\n",
    "    data['check'] = data.apply(lambda i: i.section_text[int(i.link_offset_start):int(i.link_offset_end)], axis=1)\n",
    "    # drop empty anchors\n",
    "    data = data[data['check']!='']\n",
    "    # check for hanging whitespace at start\n",
    "    wrong_link_offsets = data[data['check'].str.contains(r'^ ')] \n",
    "    data.loc[data['check'].str.contains(r'^ '), 'link_offset_start'] = [i+1 for i in wrong_link_offsets['link_offset_start'].values]\n",
    "    # check for hanging whitespace at end\n",
    "    wrong_link_offsets = data[data['check'].str.contains(r' $')] \n",
    "    data.loc[data['check'].str.contains(r' $'), 'link_offset_end'] = [i-1 for i in wrong_link_offsets['link_offset_end'].values]        \n",
    "    return data\n",
    "\n",
    "def replace_accents(text, unicode_dict):\n",
    "    import re\n",
    "    for rep, mat in unicode_dict.items():\n",
    "        text = re.sub(mat, rep, text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def adjust_entity_idx(text, replace_start, replace_end, replace_word):\n",
    "    \"\"\"\n",
    "    Given a text, and a given start and end position of the original text to replace, the text to replace it by, \n",
    "    return a list consisting of the new text, an offset to shift the characters of the text, and the character beyond \n",
    "    which an offset is required\n",
    "    \"\"\"\n",
    "\n",
    "    offset = replace_end - replace_start - len(replace_word)\n",
    "    new_text = text[:replace_start] + replace_word + text[replace_end:]\n",
    "    # original index > replace_end needs to be offset\n",
    "    return [new_text, offset]\n",
    "\n",
    "def text_preprocessing(data, regex_ls, unicode_dict):\n",
    "    \"\"\"Given the wikipedia text dataset, preprocess the text data in the column 'section_text'\n",
    "    given a regex list to remove from the text data, and then adjust the corresponding link offsets\n",
    "    under the columns 'link_offset_start', and 'link_offset_end'.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    data = data.copy()\n",
    "    \n",
    "    # existing data has some wrong link offsets initially, these have additional spaces in front of the link anchors\n",
    "    data = correct_whitespace_offset(data)\n",
    "    # sort data first by section id, then by link offset\n",
    "    data.sort_values(['source_section_id', 'link_offset_start'], inplace=True)\n",
    "    \n",
    "    # unidecode to replace accents\n",
    "    data.loc[:,'section_text'] = data.apply(lambda i: replace_accents(i.section_text, unicode_dict), axis=1)\n",
    "    data.loc[:,'link_anchor'] = data.apply(lambda i: replace_accents(i.link_anchor, unicode_dict), axis=1)\n",
    "    \n",
    "    # data cleaning while keeping track of link offsets\n",
    "    for sid in data['source_section_id'].unique():\n",
    "        replace_section = data[data['source_section_id'] == sid]\n",
    "        text = replace_section['section_text'].iloc[0]\n",
    "        # list of original link offsets\n",
    "        link_offset_idx = list(replace_section.apply(lambda i: (i.link_offset_start, i.link_offset_end), axis=1))\n",
    "    \n",
    "        # remove text based on regex\n",
    "        for regex in regex_ls:\n",
    "            match_idx = [(m.start(0), m.end(0)) for m in re.finditer(regex[0], text)]\n",
    "            if not match_idx:\n",
    "                continue\n",
    "    \n",
    "            offset = 0\n",
    "            for i in match_idx:\n",
    "                replace_ls = adjust_entity_idx(text, \n",
    "                                               i[0]-offset,\n",
    "                                               i[1]-offset,\n",
    "                                               regex[1])\n",
    "        \n",
    "                \n",
    "                # adjust link offsets\n",
    "                for j, idx in enumerate(link_offset_idx):\n",
    "                    # account for matched item within entity\n",
    "                    if (i[0]-offset>=idx[0]) & (i[1]-offset<=idx[1]):\n",
    "                        link_offset_idx[j] = (idx[0], idx[1]-replace_ls[1])\n",
    "                    # original index > replace_end needs to be offset\n",
    "                    elif idx[0] >= i[1]-offset:\n",
    "                        link_offset_idx[j] = (idx[0]-replace_ls[1], idx[1]-replace_ls[1])\n",
    "                    else:\n",
    "                        link_offset_idx[j] = (idx[0], idx[1])\n",
    "                        \n",
    "                # text preprocessing\n",
    "                text = replace_ls[0]        \n",
    "                offset += replace_ls[1]\n",
    "   \n",
    "        # edit original \n",
    "        # replace numbers with hash #\n",
    "        # https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/\n",
    "#         data.loc[data['source_section_id'] == sid, 'section_text'] = re.sub('[0-9]', '#', text)\n",
    "        data.loc[data['source_section_id'] == sid, 'section_text'] = text\n",
    "        data.loc[data['source_section_id'] == sid, 'link_offset_start'] = [i[0] for i in link_offset_idx]\n",
    "        data.loc[data['source_section_id'] == sid, 'link_offset_end'] = [i[1] for i in link_offset_idx]\n",
    "        \n",
    "    # a removed matched entity may leave whitespaces when we calculate offsets, this corrects for it\n",
    "    # existing data has some wrong link offsets initially, these have additional spaces in front/behind the link anchors\n",
    "    data = correct_whitespace_offset(data)\n",
    "    \n",
    "    data['link_offset_end'] = data['link_offset_end'].astype(int)\n",
    "    data['link_offset_start'] = data['link_offset_start'].astype(int)\n",
    "    \n",
    "    return data\n",
    "        \n",
    "# replace html encoded strings, weird characters, and additional whitespaces\n",
    "cleaned_sample = text_preprocessing(sample, [('&\\w+;|&#[0-9]+;|&#[xX][a-fA-F0-9]+;', ''), \n",
    "                                             ('[^a-zA-Z0-9\\s]', ''), \n",
    "                                             ('\\s{2,}', ' '),\n",
    "                                             ('^ | $', '')], unicode_dict)\n",
    "cleaned_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sample.to_csv('../data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data of wikidata entries\n",
    "# wikidata = pd.read_csv('../data/wikipages_cleaned.csv')\n",
    "# wikidata.dropna(inplace=True)\n",
    "\n",
    "# # replace words within paranthesis and remove underscores\n",
    "# # get_rid of `the` which can cause difference\n",
    "# pattern = re.compile(r'\\([^)]*\\)|_')\n",
    "# clean_text = lambda text: re.sub(r'\\([^)]*\\)', \"\", text).replace('_', ' ').strip()\n",
    "# wikidata.loc[:,'target_page_title'] = wikidata.apply(lambda i: clean_text(i['target_page_title']), axis=1)\n",
    "\n",
    "# # we need to replace link anchors with actual wikidata page titles\n",
    "# def replace_anchor_text(text, replace_text, replace_start, replace_end):\n",
    "#     \"\"\"\n",
    "#     Given a text, a given phrase/word to replace into the text, and a given start and end position\n",
    "#     of the original text to replace, return a list consisting of the new text, and the new start and end\n",
    "#     indexes corresponding to the replaced phrase/word, and an offset to shift the characters of the text\n",
    "#     \"\"\"\n",
    "#     import string\n",
    "#     new_replace_end = replace_start+len(replace_text)\n",
    "#     # replace full words\n",
    "#     if len(text) > replace_end:\n",
    "#         while text[replace_end] in string.ascii_letters:\n",
    "#             replace_end+=1\n",
    "#     offset = new_replace_end - replace_end\n",
    "#     new_text = text[:replace_start] + replace_text + text[replace_end:]\n",
    "#     return [new_text, replace_start, new_replace_end, offset]\n",
    "\n",
    "# # get corresponding wikidata page titles\n",
    "\n",
    "# merged_sample = sample.merge(wikidata[['wikidata_numeric_id', 'target_page_title']], \n",
    "#                                'left', left_on='target_wikidata_numeric_id', right_on='wikidata_numeric_id')\n",
    "# # SOMEHOW WHEN IT MERGES, IT DUPLICATES ALOT OF ROWS\n",
    "# merged_sample.drop_duplicates(inplace=True)\n",
    "\n",
    "# replaced_sample = []\n",
    "# # replace link anchors with actual wikidata page titles\n",
    "# # extract replaced text and indexes\n",
    "# for sid in merged_sample['source_section_id'].unique():\n",
    "#     replace_section = merged_sample[merged_sample['source_section_id'] == sid].sort_values('link_offset_start').reset_index(drop=True)\n",
    "#     text = replace_section['section_text'].loc[0]\n",
    "#     offset = 0\n",
    "#     for i in np.arange(replace_section.shape[0]):\n",
    "#         replace_ls = replace_anchor_text(text, replace_section.loc[i,'target_page_title'], \n",
    "#                                          replace_section.loc[i,'link_offset_start']+offset,\n",
    "#                                          replace_section.loc[i,'link_offset_end']+offset)\n",
    "#         text = replace_ls[0]\n",
    "#         replace_section.loc[i, 'link_offset_start'] = replace_ls[1]\n",
    "#         replace_section.loc[i, 'link_offset_end'] = replace_ls[2]\n",
    "# #     replace_section.loc[i, 'link_anchor'] = replace_section.loc[i,'target_page_title']\n",
    "#         offset += replace_ls[3]\n",
    "#     replace_section['section_text'] = text\n",
    "#     replaced_sample.append(replace_section)\n",
    "    \n",
    "# # get back dataframe\n",
    "# replaced_sample = pd.concat(replaced_sample)\n",
    "# # edit section length and link anchors\n",
    "# replaced_sample.loc[:,'link_anchor'] = replaced_sample.loc[:,'target_page_title']\n",
    "# replaced_sample.loc[:,'section_len'] = replaced_sample.apply(lambda i: len(i['section_text']), axis=1)\n",
    "\n",
    "# # drop redundant columns\n",
    "# replaced_sample.drop(['wikidata_numeric_id', 'target_page_title'], axis=1, inplace=True)\n",
    "\n",
    "# # check whether link anchor corresponds to position in text\n",
    "# check_anchor = lambda row_data: row_data['section_text'][row_data['link_offset_start']:row_data['link_offset_end']] == row_data['link_anchor']\n",
    "# assert all(replaced_sample.apply(check_anchor, axis=1))\n",
    "# assert len(replaced_sample['section_text'].unique())==5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/linguistic-features#native-tokenizers\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_and_labels_helper(section, tokenized_text):\n",
    "    \"\"\"\n",
    "    Helper function for extract_entities_and_labels\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    tokenized_vector = np.zeros(len(tokenized_text))\n",
    "    # get span object to match characters with tokens\n",
    "    char_to_token = tokenized_text.char_span(section.link_offset_start, section.link_offset_end)\n",
    "    \n",
    "    # label corresponding tokens if there is an anchor link\n",
    "    # to match anchor text rather than anchor link\n",
    "    if not char_to_token:\n",
    "        char_tokens = np.array([token.idx for token in tokenized_text])\n",
    "        # to account for tokens at end of text\n",
    "        if np.where(char_tokens >= section.link_offset_end)[0].size > 0:\n",
    "            closest_start_token = char_tokens[np.where(char_tokens <= section.link_offset_start)[0][-1]]\n",
    "            closest_end_token = char_tokens[np.where(char_tokens >= section.link_offset_end)[0][0]]-1\n",
    "            char_to_token = tokenized_text.char_span(closest_start_token, closest_end_token)\n",
    "            tokenized_vector[char_to_token.start:char_to_token.end] = 1\n",
    "        else:\n",
    "            tokenized_vector[np.where(char_tokens <= section.link_offset_start)[0][-1]:-1] = 1\n",
    "    else:\n",
    "        tokenized_vector[char_to_token.start:char_to_token.end] = 1 \n",
    "        \n",
    "    # extract other relevant information\n",
    "    data = {(section.link_anchor, section.link_offset_start, \n",
    "             section.link_offset_end, \n",
    "             section.target_wikidata_numeric_id): tokenized_vector}\n",
    "    return data\n",
    "\n",
    "def drop_no_entities(ner_data, label_data):\n",
    "    \"\"\"\n",
    "    Given NER data and true labels data, both in the format\n",
    "    i.e. [['Apple is a good company', {('Apple', 0, 6, 0101102086): [1,0,0,0,0]}],...]\n",
    "    check if there are text documents with either no entities identified by NER, or no\n",
    "    entities with Wikidata links. Drop all of these text documents, and returns the NER \n",
    "    and true labels data.\n",
    "    \"\"\"\n",
    "    # check to drop all texts without any ner_entity, or any true entity\n",
    "    ner_entities_missing = [i for i, text in enumerate(ner_data) if not text[1]]\n",
    "    true_entities_missing = [i for i, text in enumerate(label_data) if not text[1]]\n",
    "    \n",
    "    # drop entries if either does not have any entities\n",
    "    new_ner_data = [text for i, text in enumerate(ner_data) if i not in ner_entities_missing+true_entities_missing]\n",
    "    new_label_data = [text for i, text in enumerate(label_data) if i not in ner_entities_missing+true_entities_missing]\n",
    "    return new_ner_data, new_label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_and_labels(data):\n",
    "    \"\"\"\n",
    "    Given a data of wikipedia articles, extract two items, both in the format of a list of lists.\n",
    "    Each entry in the outer list corresponds to a document. \n",
    "    For each document (inner list), the first entry is the actual text document, while the second entry is a dictionary.\n",
    "    For the first item extracted, the dictionary has the true entities (wikidata links), \n",
    "    start characters, end characters, and true Wikidata entry IDs as the key of the dictionary.\n",
    "    The dictionary has a vector of zeros, with ones at the positions of the corresponding token positions of the true entities\n",
    "    i.e. ['Apple is a good company', {('Apple', 0, 6, 0101102086): [1,0,0,0,0]}]\n",
    "    For the second item extracted, the item is largely the same. However, they contain the NER identified entities rather than\n",
    "    the true entities. In the position of the true Wikidata entry IDs, they have an entity label instead.\n",
    "    i.e. ['Apple is a good company', {('Apple', 0, 6, 'ORG'): [1,0,0,0,0]}]\n",
    "    \"\"\"\n",
    "    import spacy\n",
    "    import re\n",
    "    import numpy as np\n",
    "    true_entity = []\n",
    "    ner_entity = []\n",
    "    \n",
    "    # for each section of text\n",
    "    for sid in data['source_section_id'].unique():\n",
    "        text = data.loc[data['source_section_id']==sid, 'section_text'].iloc[0]   \n",
    "        \n",
    "        # tokenizer+NER\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "        tokenized_text = nlp(text)\n",
    "        \n",
    "        # NER entities\n",
    "        ner_entity_dict = {}\n",
    "        for entity in tokenized_text.ents:\n",
    "            if entity.label_ in ['DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']:\n",
    "                continue\n",
    "            ner_tokenized_vector = np.zeros(len(tokenized_text))\n",
    "            char_to_token = tokenized_text.char_span(entity.start_char, entity.end_char)\n",
    "            ner_tokenized_vector[char_to_token.start:char_to_token.end] = 1        \n",
    "            ner_entity_dict[(entity.text, entity.start_char, entity.end_char, entity.label_)] = ner_tokenized_vector\n",
    "    \n",
    "        \n",
    "        # actual wikidata links entities     \n",
    "        # extract relevant data from dataframe \n",
    "        entity_data = (data[data['source_section_id']==sid]\n",
    "                       .apply(lambda i: extract_entities_and_labels_helper(i, tokenized_text), axis=1))\n",
    "              \n",
    "        # form the correct data structure\n",
    "        # wikidata link true entity\n",
    "        # replace numbers with hash #\n",
    "        # https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/\n",
    "        # these were originally kept for NER purposes\n",
    "        new_text = re.sub('[0-9]', '#', text.lower())\n",
    "        true_entity.append([new_text, dict()])\n",
    "        for entity in list(entity_data):\n",
    "            true_entity[-1][-1].update(entity)\n",
    "            \n",
    "        # ner identified entity\n",
    "        ner_entity.append([new_text, ner_entity_dict])\n",
    "    \n",
    "    # drop text documents with either no entities identified by NER, or no entities with Wikidata links\n",
    "    ner_entity, true_entity = drop_no_entities(ner_entity, true_entity)\n",
    "    \n",
    "    return true_entity, ner_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approx 10-15 minutes for 5000 text...\n",
    "true_entity, ner_entity = extract_entities_and_labels(cleaned_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/sample_full_labels.pkl', 'wb') as f:\n",
    "#     pickle.dump(ylabel, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('../data/sample_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(sample_ner, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now extract the NER entities that also have a corresponding 'true' Wikidata link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_01_vector(data, axis):\n",
    "    \"\"\"\n",
    "    Given data in the format \n",
    "    i.e. [['Apple is a good company', {('Apple', 0, 6, 0101102086): [1,0,0,0,0]}],...]\n",
    "    extract all the 0-1 vectors within each dictionary, \n",
    "    concatenate them together along a given axis, and repeat for every single entry in the data, \n",
    "    where each entry in the data corresponds to a text document. \n",
    "    The function then returns 2 items, the first a list of arrays corresponding to the concatenated\n",
    "    0-1 vectors, the second a list (of the same length) of dictionaries where the key, value pairs \n",
    "    correspond to the index within the arrays (from the 1st item) and the matching key pair of entity\n",
    "    matches from the data respectively\n",
    "    \"\"\"\n",
    "    key_idx_dict_ls = []\n",
    "    concat_vec_all = []\n",
    "    for text in data:\n",
    "        key_idx_dict = {}\n",
    "        concat_vec = []\n",
    "        # to check if there are entities in the data\n",
    "        if not text[1]:\n",
    "            raise Exception('Drop text documents without entities!')\n",
    "        else:\n",
    "            for i, (key, val) in enumerate(text[1].items()):\n",
    "                concat_vec.append(val)\n",
    "                key_idx_dict[i] = key\n",
    "            concat_vec = np.stack(concat_vec, axis=axis)\n",
    "\n",
    "            # each entry in the list corresponds to a text\n",
    "            concat_vec_all.append(concat_vec)\n",
    "            key_idx_dict_ls.append(key_idx_dict)\n",
    "    return concat_vec_all, key_idx_dict_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_entities(ner_entity, true_entity):\n",
    "    # for each identified NER entity in each text\n",
    "    ner_entities, ner_key_idx_dict_ls = extract_01_vector(ner_entity, axis=1)\n",
    "    # possible true entities with wikidata links\n",
    "    true_entities, true_key_idx_dict_ls = extract_01_vector(true_entity, axis=0)\n",
    "    entity_check_idx_ls = []\n",
    "    for i in np.arange(len(true_entities)):\n",
    "        # FOR NOW, AS LONG AS THERE IS SOME OVERLAP, WE KEEP THE TRUE ENTITY\n",
    "        entity_check = np.sum(true_entities[i] @ ner_entities[i], axis=1)\n",
    "    \n",
    "        # entities which have a Wikidata link, but do not have a corresponding NER entity\n",
    "        entity_check_idx = np.where(entity_check==0)[0]\n",
    "        entity_check_idx_ls.append(entity_check_idx)\n",
    "        \n",
    "    for data, idx_ls, key_idx in zip(true_entity, entity_check_idx_ls, true_key_idx_dict_ls):\n",
    "        for i in idx_ls:\n",
    "            del data[1][key_idx[i]]\n",
    "        \n",
    "    ner_entity, true_entity = drop_no_entities(ner_entity, true_entity)        \n",
    "    # possible true entities with wikidata links\n",
    "    # for each identified NER entity in each text\n",
    "    ner_entities, ner_key_idx_dict_ls = extract_01_vector(ner_entity, axis=1)\n",
    "    true_entities, true_key_idx_dict_ls = extract_01_vector(true_entity, axis=0)\n",
    "    entity_check_idx_ls = []\n",
    "    for i in np.arange(len(ner_entities)):\n",
    "        # FOR NOW, AS LONG AS THERE IS SOME OVERLAP, WE KEEP THE TRUE ENTITY\n",
    "        entity_check = np.sum(true_entities[i] @ ner_entities[i], axis=0)\n",
    "    \n",
    "        # entities which have a Wikidata link, but do not have a corresponding NER entity\n",
    "        entity_check_idx = np.where(entity_check==0)[0]\n",
    "        entity_check_idx_ls.append(entity_check_idx)\n",
    "        \n",
    "    # drop all those NER entities with no corresponding wikidata link\n",
    "    for data, idx_ls, key_idx in zip(ner_entity, entity_check_idx_ls, ner_key_idx_dict_ls):\n",
    "        for i in idx_ls:\n",
    "            del data[1][key_idx[i]]\n",
    "\n",
    "    # drop document if there is no entity\n",
    "    ner_entity, true_entity = drop_no_entities(ner_entity, true_entity)\n",
    "    return ner_entity, true_entity\n",
    "\n",
    "ner_entity, true_entity = drop_entities(ner_entity, true_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of True Entities identified by NER: 0.646969742859739\n"
     ]
    }
   ],
   "source": [
    "# # for each identified NER entity in each text\n",
    "# ner_entities, ner_key_idx_dict_ls = extract_01_vector(ner_entity, axis=1)\n",
    "# # possible true entities with wikidata links\n",
    "# true_entities, true_key_idx_dict_ls = extract_01_vector(true_entity, axis=0)\n",
    "\n",
    "# # for each true entity in each text, check for a corresponding identified NER entity\n",
    "# # if there is no corresponding identified NER entity, drop the true entity\n",
    "# # check for the percentage of entities correctly identified by NER\n",
    "# n_true_ent = 0\n",
    "# n_ner_ent = 0\n",
    "# entity_check_idx_ls = []\n",
    "# for i in np.arange(len(true_entities)):\n",
    "#     # FOR NOW, AS LONG AS THERE IS SOME OVERLAP, WE KEEP THE TRUE ENTITY\n",
    "#     entity_check = np.sum(true_entities[i] @ ner_entities[i], axis=1)\n",
    "    \n",
    "#     # entities which have a Wikidata link, but do not have a corresponding NER entity\n",
    "#     entity_check_idx = np.where(entity_check==0)[0]\n",
    "#     entity_check_idx_ls.append(entity_check_idx)\n",
    "    \n",
    "#     # percentage of entities identified\n",
    "#     n_true_ent += entity_check.shape[0]\n",
    "#     n_ner_ent += entity_check.shape[0]-len(entity_check_idx)\n",
    "\n",
    "# print('Percentage of True Entities identified by NER: {}'.format(n_ner_ent/n_true_ent))\n",
    "\n",
    "# # drop all those not identified entities in the true entities list\n",
    "# # note that we did not drop NER identified entities without a corresponding true entity\n",
    "# for data, idx_ls, key_idx in zip(true_entity, entity_check_idx_ls, true_key_idx_dict_ls):\n",
    "#     for i in idx_ls:\n",
    "#         del data[1][key_idx[i]]\n",
    "        \n",
    "# ner_entity, true_entity = drop_no_entities(ner_entity, true_entity)        \n",
    "# # possible true entities with wikidata links\n",
    "# # for each identified NER entity in each text\n",
    "# ner_entities, ner_key_idx_dict_ls = extract_01_vector(ner_entity, axis=1)\n",
    "# true_entities, true_key_idx_dict_ls = extract_01_vector(true_entity, axis=0)\n",
    "\n",
    "# # for each NER entity in each text, check for a corresponding true entity\n",
    "# # if there is no corresponding true entity, drop the NER entity\n",
    "# n_true_ent = 0\n",
    "# n_ner_ent = 0\n",
    "# entity_check_idx_ls = []\n",
    "# for i in np.arange(len(ner_entities)):\n",
    "#     # FOR NOW, AS LONG AS THERE IS SOME OVERLAP, WE KEEP THE TRUE ENTITY\n",
    "#     entity_check = np.sum(true_entities[i] @ ner_entities[i], axis=0)\n",
    "    \n",
    "#     # entities which have a Wikidata link, but do not have a corresponding NER entity\n",
    "#     entity_check_idx = np.where(entity_check==0)[0]\n",
    "#     entity_check_idx_ls.append(entity_check_idx)\n",
    "    \n",
    "#     # percentage of entities identified\n",
    "#     n_true_ent += entity_check.shape[0]\n",
    "#     n_ner_ent += entity_check.shape[0]-len(entity_check_idx)\n",
    "\n",
    "# print('Percentage of identified NER Entities with corresponding true link: {}'.format(n_ner_ent/n_true_ent))\n",
    "\n",
    "# # drop all those NER entities with no corresponding wikidata link\n",
    "# for data, idx_ls, key_idx in zip(ner_entity, entity_check_idx_ls, ner_key_idx_dict_ls):\n",
    "#     for i in idx_ls:\n",
    "#         del data[1][key_idx[i]]\n",
    "\n",
    "# # drop document if there is no entity\n",
    "# ner_entity, true_entity = drop_no_entities(ner_entity, true_entity)        \n",
    "# # for each identified NER entity in each text\n",
    "# ner_entities, ner_key_idx_dict_ls = extract_01_vector(ner_entity, axis=1)\n",
    "# # possible true entities with wikidata links\n",
    "# true_entities, true_key_idx_dict_ls = extract_01_vector(true_entity, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, drop NER entities without corresponding hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sample_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(true_entity, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../data/sample_data.pkl', 'wb') as f:\n",
    "    pickle.dump(ner_entity, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
    "# get list of documents\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "docs = []\n",
    "for text in ner_entity:\n",
    "    docs.append(text[0])\n",
    "tagged_docs = [TaggedDocument(words=word_tokenize(word), tags=[str(i)]) for i, word in enumerate(docs)]\n",
    "\n",
    "max_epochs = 100\n",
    "vec_size = 1000\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size = vec_size,\n",
    "                alpha = alpha,\n",
    "                min_alpha = 0.0025,\n",
    "                min_count = 1,\n",
    "                dm = 1)\n",
    "model.build_vocab(tagged_docs)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('Iteration {}'.format(epoch))\n",
    "    model.train(tagged_docs,\n",
    "               total_examples=model.corpus_count,\n",
    "               epochs=model.epochs)\n",
    "    # decrease learning rate over time\n",
    "    model.alpha -= 0.0002\n",
    "    # ensure no decay\n",
    "    model.min_alpha = model.alpha\n",
    "# save model\n",
    "model.save('../data/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model = Doc2Vec.load('../data/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3683', 0.6052637100219727),\n",
       " ('1395', 0.6032894849777222),\n",
       " ('1399', 0.6031019687652588),\n",
       " ('3139', 0.5970553159713745),\n",
       " ('2870', 0.5903152227401733),\n",
       " ('1456', 0.5884307622909546),\n",
       " ('2714', 0.588376522064209),\n",
       " ('3467', 0.5871361494064331),\n",
       " ('2715', 0.5861045122146606),\n",
       " ('3593', 0.5852940082550049)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar('6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transport in north korea is constrained by economic problems and government restrictions public transport predominates and most of it is electrified'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'annerley road is an arterial road in brisbane queensland australia it was formerly known as boggo road'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3683]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match ner entity with true corresponding entity\n",
    "true_entity_idx_ls = []\n",
    "# for each text\n",
    "for i in np.arange(len(ner_entities)):\n",
    "    entity_arr = true_entities[i] @ ner_entities[i]\n",
    "    true_entity_idx = []\n",
    "    for j in np.arange(entity_arr.shape[1]):\n",
    "        true_entity_idx.append(list(np.where(entity_arr[:,j]>0)[0]))\n",
    "    true_entity_idx_ls.append(true_entity_idx)\n",
    "    \n",
    "# # form doc2vec data\n",
    "text_doc2vec_ls = []\n",
    "for i, (ner_dict, idx_ls, true_dict) in enumerate(zip(ner_key_idx_dict_ls, true_entity_idx_ls, true_key_idx_dict_ls)):\n",
    "    text_doc2vec = [model.docvecs[str(i)], dict()]\n",
    "    for j, idx in enumerate(idx_ls):\n",
    "        for k in idx:\n",
    "            text_doc2vec[-1][(ner_dict[j][0], true_dict[k][0])] = true_dict[k][-1]\n",
    "    text_doc2vec_ls.append(text_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sample_doc2vec_data.pkl', 'wb') as f:\n",
    "    pickle.dump(text_doc2vec_ls, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
