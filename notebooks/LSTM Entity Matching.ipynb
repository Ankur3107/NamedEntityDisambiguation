{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Input, Concatenate, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intros\n",
    "\n",
    "with open('../data/sample_labels.pkl', 'rb') as f:\n",
    "    intros = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entity embeddings for each entity ID\n",
    "\n",
    "with open('../data/knowledge_graph_data/wiki_DistMult_entity.npy', 'rb') as f:\n",
    "    e = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entity id to embedding mapping. This is so we can retrieve the \n",
    "#entity embeddings when we know the index of the entity\n",
    "\n",
    "with open('../data/knowledge_graph_data/idx2id_entity_full_no_text.pickle', 'rb') as f:\n",
    "    idx2id = pickle.load(f)\n",
    "id2idx = {v: k for k, v in idx2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break intros into lists of the text, entity locations, and entity IDs\n",
    "\n",
    "num_entities = 0\n",
    "full_text = []\n",
    "entity_locations = []\n",
    "entity_id = []\n",
    "\n",
    "for intro in intros:\n",
    "    if intro[1]:\n",
    "        full_text.append(intro[0])\n",
    "        \n",
    "        temp = []\n",
    "        temp1 = []\n",
    "        \n",
    "        for idx,entity_key in enumerate(intro[1]):\n",
    "\n",
    "            temp.append(entity_key[3])\n",
    "            loc = np.argwhere(intro[1][entity_key]==1)\n",
    "            temp1.append((loc.min(),loc.max()))\n",
    "            num_entities+=1\n",
    "                \n",
    "        entity_id.append(temp)\n",
    "        entity_locations.append(temp1)\n",
    "\n",
    "full_text = np.asarray(full_text)\n",
    "entity_locations = np.asarray(entity_locations)\n",
    "entity_id = np.asarray(entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create paragraphs array.\n",
    "#The paragraph array is a list of sublists. Each sublist is a list of words contained in the paragraph.\n",
    "\n",
    "\n",
    "paragraphs = []\n",
    "for paragraph in full_text:\n",
    "    temp = []\n",
    "    for sentence in sent_tokenize(paragraph):\n",
    "        for word in word_tokenize(sentence):\n",
    "            temp.append(word)\n",
    "    paragraphs.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#train w2v model and create intro array\n",
    "w2v_size = 100\n",
    "w2v_model = Word2Vec(paragraphs, min_count = 1, size = w2v_size, window = 5, sg=1)\n",
    "vocab_size = len(w2v_model.wv.vocab)\n",
    "\n",
    "intro_vectors = []\n",
    "for sentence in paragraphs:\n",
    "    temp = []\n",
    "    for word in sentence:\n",
    "        temp.append(w2v_model[word])\n",
    "    intro_vectors.append(temp)\n",
    "\n",
    "intro_vectors = [[l.tolist() for l in vectors] for vectors in intro_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent success: 88.41346378914845\n"
     ]
    }
   ],
   "source": [
    "#get training data to be used for LSTM\n",
    "#X will be a list of sublists. Each sublist contains the vectors of words in the context window for each entity.\n",
    "\n",
    "context_window = 10 #number of words with entity centered for input to LSTM model\n",
    "\n",
    "#using text sequences\n",
    "X_words = []\n",
    "X_w2v = []\n",
    "Y = []\n",
    "count_fail=0\n",
    "count_success=0\n",
    "\n",
    "for idx,locations in enumerate(entity_locations):\n",
    "    for idx2,loc in enumerate(locations):\n",
    "        low = max(loc[0]-context_window//2,0)\n",
    "        r_extra = max(0,context_window//2-loc[0])\n",
    "        high = min(loc[0]+context_window//2,len(paragraphs[idx]))\n",
    "        l_extra = max(loc[0]+context_window//2-len(paragraphs[idx]),0)\n",
    "        try:\n",
    "            Y.append(e[id2idx[entity_id[idx][idx2]]])\n",
    "            X_words.append(paragraphs[idx][low-l_extra:high+r_extra])\n",
    "            X_w2v.append(intro_vectors[idx][low-l_extra:high+r_extra])\n",
    "            count_success+=1\n",
    "        except:\n",
    "            count_fail+=1\n",
    "Y_array = np.zeros((len(Y),Y[0].shape[0]))\n",
    "for idx,y in enumerate(Y):\n",
    "    for idx2,y2 in enumerate(y):\n",
    "        Y_array[idx][idx2] = y2\n",
    "Y = Y_array\n",
    "print('Percent success: {}'.format(100*(count_success/(count_success+count_fail))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We cannot feed words into the LSTM. So we need to tokenize the words\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_words)\n",
    "X_token_words = np.zeros((len(X_words),context_window), dtype=int)\n",
    "for idx,window in enumerate(X_words):\n",
    "    for idx2,word in enumerate(window):\n",
    "        X_token_words[idx][idx2] = t.word_counts[word]\n",
    "num_unique_words = X_token_words.max()+1\n",
    "\n",
    "#Convert X_w2v list into array\n",
    "X_w2v_new = np.zeros((len(X_w2v),context_window,w2v_size))\n",
    "for idx,window in enumerate(X_w2v):\n",
    "    for idx2,word in enumerate(window):\n",
    "        for idx3,emb in enumerate(word):\n",
    "            X_w2v_new[idx][idx2][idx3] = emb\n",
    "X_w2v = X_w2v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words,X_test_words,X_train_w2v,X_test_w2v,Y_train,Y_test = train_test_split(X_token_words,X_w2v,Y)\n",
    "X_train = [X_train_words,X_train_w2v]\n",
    "X_test = [X_test_words,X_test_w2v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer,loss,metrics):\n",
    "    \n",
    "    #inputs\n",
    "    inp_context_words = Input(shape = (context_window,), name='inp_context_words')\n",
    "    inp_w2v = Input(shape = (context_window,w2v_size), name = 'inp_w2v')\n",
    "    \n",
    "    #embed the context words\n",
    "    emb = Embedding(output_dim = 100, input_dim = num_unique_words, input_length = context_window,\n",
    "                   name = 'emb1')(inp_context_words)\n",
    "    emb = Dropout(0.2, name = 'emb2')(emb)\n",
    "    \n",
    "    #LSTM input\n",
    "    lstm_inp = concatenate([inp_w2v,emb], axis = 2, name = 'lstm_inp')\n",
    "    \n",
    "    lstm_1 = Bidirectional(LSTM(500,name = 'lstm_layer'))(lstm_inp)\n",
    "    hidden_1 = Dropout(0.2, name = 'hidden_2')(lstm_1)\n",
    "    \n",
    "    output = Dense(e[0].shape[0],activation='linear',name = 'output')(hidden_1)\n",
    "    \n",
    "    model = Model(inputs=[inp_context_words,inp_w2v],outputs = output)\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp_context_words (InputLayer)  (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb1 (Embedding)                (None, 10, 100)      2248300     inp_context_words[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inp_w2v (InputLayer)            (None, 10, 100)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb2 (Dropout)                  (None, 10, 100)      0           emb1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_inp (Concatenate)          (None, 10, 200)      0           inp_w2v[0][0]                    \n",
      "                                                                 emb2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1000)         2804000     lstm_inp[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hidden_2 (Dropout)              (None, 1000)         0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 250)          250250      hidden_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,302,550\n",
      "Trainable params: 5,302,550\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = \"adam\"\n",
    "loss = \"MSE\"\n",
    "metrics = [\"MSE\"]\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "validation_split = 0.1\n",
    "verbose = 1\n",
    "\n",
    "model = create_model(optimizer,loss,metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12747 samples, validate on 1417 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "LSTM_history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_split=validation_split, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set accuracy\n",
    "y_test_emb = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
