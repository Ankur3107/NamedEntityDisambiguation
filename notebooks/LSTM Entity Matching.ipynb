{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Bidirectional, Embedding, Input, Concatenate\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intros\n",
    "\n",
    "with open('../data/sample_labels.pkl', 'rb') as f:\n",
    "    intros = pickle.load(f)\n",
    "    \n",
    "#with open('../data/sample_data.pkl', 'rb') as f:\n",
    "    #intros = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entity embeddings for each entity ID\n",
    "\n",
    "with open('../data/knowledge_graph_data/wiki_DistMult_entity.npy', 'rb') as f:\n",
    "    e = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entity id to embedding mapping\n",
    "with open('../data/knowledge_graph_data/idx2id_entity_full_no_text.pickle', 'rb') as f:\n",
    "    idx2id = pickle.load(f)\n",
    "id2idx = {v: k for k, v in idx2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break intros into lists of the text, entity locations, and entity IDs\n",
    "\n",
    "full_text = []\n",
    "entity_locations = []\n",
    "entity_id = []\n",
    "\n",
    "for intro in intros:\n",
    "    if intro[1]:\n",
    "        full_text.append(intro[0])\n",
    "        \n",
    "        temp = []\n",
    "        temp1 = []\n",
    "        \n",
    "        for idx,entity_key in enumerate(intro[1]):\n",
    "\n",
    "            temp.append(entity_key[3])\n",
    "            loc = np.argwhere(intro[1][entity_key]==1)\n",
    "            temp1.append((loc.min(),loc.max()))\n",
    "                \n",
    "        entity_id.append(temp)\n",
    "        entity_locations.append(temp1)\n",
    "\n",
    "full_text = np.asarray(full_text)\n",
    "entity_locations = np.asarray(entity_locations)\n",
    "entity_id = np.asarray(entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create words/paragraphs arrays\n",
    "\n",
    "words = []\n",
    "paragraphs = []\n",
    "for paragraph in full_text:\n",
    "    temp1 = []\n",
    "    for sentence in sent_tokenize(paragraph):\n",
    "        temp = []\n",
    "        for word in word_tokenize(sentence):\n",
    "            temp.append(word)\n",
    "            temp1.append(word)\n",
    "        words.append(temp)\n",
    "    paragraphs.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#train w2v model and create intro array\n",
    "w2v_model = Word2Vec(words, min_count = 1, size = 100, window = 5, sg=1)\n",
    "\n",
    "intro_vectors = []\n",
    "for sentence in paragraphs:\n",
    "    temp = []\n",
    "    for word in sentence:\n",
    "        temp.append(w2v_model[word])\n",
    "    intro_vectors.append(temp)\n",
    "\n",
    "intro_vectors = [[l.tolist() for l in vectors] for vectors in intro_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent success: 88.41346378914845\n"
     ]
    }
   ],
   "source": [
    "#get training data\n",
    "#X will be a list of sublists. Each sublist contains the vectors of words in the context window for each entity.\n",
    "\n",
    "context_window = 40 #number of words with entity centered for input to LSTM model\n",
    "#note: also try including all of intro up to context word (and perhaps few words to the right)\n",
    "\n",
    "#using text sequences\n",
    "X_words = []\n",
    "X_w2v = []\n",
    "Y = []\n",
    "count_fail=0\n",
    "count_success=0\n",
    "\n",
    "for idx,locations in enumerate(entity_locations):\n",
    "    for idx2,loc in enumerate(locations):\n",
    "        low = max(loc[0]-context_window//2,0)\n",
    "        r_extra = max(0,context_window//2-loc[0])\n",
    "        high = min(loc[0]+context_window//2,len(paragraphs[idx]))\n",
    "        l_extra = max(loc[0]+context_window//2-len(paragraphs[idx]),0)\n",
    "        try:\n",
    "            Y.append(e[id2idx[entity_id[idx][idx2]]])\n",
    "            X_words.append(paragraphs[idx][low-l_extra:high+r_extra])\n",
    "            X_w2v.append(intro_vectors[idx][low-l_extra:high+r_extra])\n",
    "            count_success+=1\n",
    "        except:\n",
    "            count_fail+=1\n",
    "print('Percent success: {}'.format(100*(count_success/(count_success+count_fail))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize X_words\n",
    "xi = [i for i in range(100)]\n",
    "yi = [[i for i in range(100)] for i in range(1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in yi:\n",
    "    cosine(xi,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    vocab_size = len(w2v_model.wv.vocab)\n",
    "    model1.add(Embedding(vocab_size,100,input_length=context_window))\n",
    "    \n",
    "    model2 = Input(shape=(len(X_w2v[0]),len(X_w2v[0][0])))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Concatenate([model1,model2]))\n",
    "    model.add(LSTM(10))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1102 15:05:27.136999 139724238182208 deprecation_wrapper.py:119] From /home/matteo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1102 15:05:27.145656 139724238182208 deprecation_wrapper.py:119] From /home/matteo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1102 15:05:27.147412 139724238182208 deprecation_wrapper.py:119] From /home/matteo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1102 15:05:27.155192 139724238182208 deprecation_wrapper.py:119] From /home/matteo/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = \"adam\"\n",
    "loss = \"sparse_categorical_crossentropy\"\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 1\n",
    "validation_split = 0.1\n",
    "verbose = 1\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=optimizer, loss = loss, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.socket.nbytes'\n",
      "OverflowError: value too large to convert to int\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "LSTM_history = model.fit([X_words,X_w2v], Y, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_split=validation_split, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add preprocessing step to hash (if not, then tokenize) input text (X_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
