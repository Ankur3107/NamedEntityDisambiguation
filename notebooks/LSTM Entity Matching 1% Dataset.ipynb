{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Input, Concatenate, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import backend as K\n",
    "\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = pd.read_csv('../data/sample_data_1percent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entity embeddings for each entity ID\n",
    "\n",
    "with open('../data/knowledge_graph_data/wiki_DistMult_entity.npy', 'rb') as f:\n",
    "    entity_KG_emb = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entity id to embedding mapping. This is so we can retrieve the \n",
    "#entity embeddings when we know the index of the entity\n",
    "\n",
    "with open('../data/knowledge_graph_data/idx2id_entity_full_no_text.pickle', 'rb') as f:\n",
    "    idx2id = pickle.load(f)\n",
    "id2idx = {v: k for k, v in idx2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/model1/lstm_input.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-406c6e295c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_input\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/model1/lstm_input.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/model1/lstm_input.pkl'"
     ]
    }
   ],
   "source": [
    "lstm_input  = np.load('../data/model1/lstm_input.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(data_og, context_window):\n",
    "    \n",
    "    data_og = data_og.drop(['link_anchor','link_start','link_end'],axis = 1)\n",
    "    \n",
    "    data = data_og.copy()\n",
    "    data['text_og'] = data['text']\n",
    "    data['kg_emb'] = data['target_wikidata']\n",
    "    \n",
    "    text = data_og.copy()\n",
    "    \n",
    "    for idx,row in data.iterrows():\n",
    "        \n",
    "        #tokenize the text\n",
    "        data['text_og'][idx] = word_tokenize(data['text'][idx])\n",
    "        data['text'][idx] = word_tokenize(data['text'][idx])\n",
    "        \n",
    "        try:\n",
    "            a =entity_KG_emb[id2idx[data['target_wikidata'][idx]]]\n",
    "            #data['tokenized_vector'][idx] = np.array([int(c) for c in list(row['tokenized_vector']) if c.isdigit()])\n",
    "        except:\n",
    "            print('uh oh')\n",
    "        #only include ones where entity KG exists\n",
    "        try: \n",
    "        \n",
    "            data['kg_emb'][idx] = entity_KG_emb[id2idx[data['target_wikidata'][idx]]]\n",
    "            \n",
    "            #fix indices\n",
    "            data['tokenized_vector'][idx] = np.array([int(c) for c in list(row['tokenized_vector']) if c.isdigit()])\n",
    "\n",
    "            #get context window\n",
    "            entity_idx = np.where(data['tokenized_vector'][idx]==1)[0]\n",
    "            low = max(entity_idx-context_window//2,0)\n",
    "            r_extra = max(0,context_window//2-entity_idx)\n",
    "            high = min(entity_idx+context_window//2,len(data['text'][idx]))\n",
    "            l_extra = max(entity_idx+context_window//2-len(data['text'][idx]),0)\n",
    "            data['text'][idx] = data['text'][idx][low-l_extra:high+r_extra]\n",
    "            \n",
    "            \n",
    "        except: \n",
    "            data.drop(idx, inplace=True)\n",
    "\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/matteo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/matteo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/matteo/.local/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "data1 = data_clean(data[:10],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v(text):\n",
    "    \n",
    "    w2v_size = 100\n",
    "    w2v_model = Word2Vec(text, min_count = 1, size = w2v_size, window = 5, sg=1)\n",
    "    vocab_size = len(w2v_model.wv.vocab)\n",
    "\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = train_w2v(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('w2v_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data['text'][18]\n",
    "b = data['tokenized_vector'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_windows(data, context_window):\n",
    "    context_windows = data.copy()\n",
    "    for idx,article in data.iterrows():\n",
    "        context_windows['tokenized_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create paragraphs array.\n",
    "#The paragraph array is a list of sublists. Each sublist is a list of words contained in the paragraph.\n",
    "\n",
    "\n",
    "paragraphs = []\n",
    "for paragraph in full_text:\n",
    "    temp = []\n",
    "    for sentence in sent_tokenize(paragraph):\n",
    "        for word in word_tokenize(sentence):\n",
    "            temp.append(word)\n",
    "    paragraphs.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#train w2v model and create intro array\n",
    "w2v_size = 100\n",
    "w2v_model = Word2Vec(paragraphs, min_count = 1, size = w2v_size, window = 5, sg=1)\n",
    "vocab_size = len(w2v_model.wv.vocab)\n",
    "\n",
    "intro_vectors = []\n",
    "for sentence in paragraphs:\n",
    "    temp = []\n",
    "    for word in sentence:\n",
    "        temp.append(w2v_model[word])\n",
    "    intro_vectors.append(temp)\n",
    "\n",
    "intro_vectors = [[l.tolist() for l in vectors] for vectors in intro_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent success: 88.41346378914845\n"
     ]
    }
   ],
   "source": [
    "#get training data to be used for LSTM\n",
    "#X will be a list of sublists. Each sublist contains the vectors of words in the context window for each entity.\n",
    "\n",
    "context_window = 10 #number of words with entity centered for input to LSTM model\n",
    "\n",
    "#using text sequences\n",
    "X_words = []\n",
    "X_w2v = []\n",
    "Y = []\n",
    "count_fail=0\n",
    "count_success=0\n",
    "\n",
    "for idx,locations in enumerate(entity_locations):\n",
    "    for idx2,loc in enumerate(locations):\n",
    "        low = max(loc[0]-context_window//2,0)\n",
    "        r_extra = max(0,context_window//2-loc[0])\n",
    "        high = min(loc[0]+context_window//2,len(paragraphs[idx]))\n",
    "        l_extra = max(loc[0]+context_window//2-len(paragraphs[idx]),0)\n",
    "        try:\n",
    "            Y.append(e[id2idx[entity_id[idx][idx2]]])\n",
    "            X_words.append(paragraphs[idx][low-l_extra:high+r_extra])\n",
    "            X_w2v.append(intro_vectors[idx][low-l_extra:high+r_extra])\n",
    "            count_success+=1\n",
    "        except:\n",
    "            count_fail+=1\n",
    "Y_array = np.zeros((len(Y),Y[0].shape[0]))\n",
    "for idx,y in enumerate(Y):\n",
    "    for idx2,y2 in enumerate(y):\n",
    "        Y_array[idx][idx2] = y2\n",
    "Y = Y_array\n",
    "print('Percent success: {}'.format(100*(count_success/(count_success+count_fail))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We cannot feed words into the LSTM. So we need to tokenize the words\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_words)\n",
    "X_token_words = np.zeros((len(X_words),context_window), dtype=int)\n",
    "for idx,window in enumerate(X_words):\n",
    "    for idx2,word in enumerate(window):\n",
    "        X_token_words[idx][idx2] = t.word_counts[word]\n",
    "num_unique_words = X_token_words.max()+1\n",
    "\n",
    "#Convert X_w2v list into array\n",
    "X_w2v_new = np.zeros((len(X_w2v),context_window,w2v_size))\n",
    "for idx,window in enumerate(X_w2v):\n",
    "    for idx2,word in enumerate(window):\n",
    "        for idx3,emb in enumerate(word):\n",
    "            X_w2v_new[idx][idx2][idx3] = emb\n",
    "X_w2v = X_w2v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words,X_test_words,X_train_w2v,X_test_w2v,Y_train,Y_test = train_test_split(X_token_words,X_w2v,Y)\n",
    "X_train = [X_train_words,X_train_w2v]\n",
    "X_test = [X_test_words,X_test_w2v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom loss function for cosine distance (for binary classification)\n",
    "\n",
    "def cosine_distance(ytrue,ypred):\n",
    "    return -K.mean(ytrue * ypred, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer,loss,metrics):\n",
    "    \n",
    "    #inputs\n",
    "    inp_context_words = Input(shape = (context_window,), name='inp_context_words')\n",
    "    inp_w2v = Input(shape = (context_window,w2v_size), name = 'inp_w2v')\n",
    "    \n",
    "    #embed the context words\n",
    "    emb = Embedding(output_dim = 100, input_dim = num_unique_words, input_length = context_window,\n",
    "                   name = 'emb1')(inp_context_words)\n",
    "    emb = Dropout(0.2, name = 'emb2')(emb)\n",
    "    \n",
    "    #LSTM input\n",
    "    lstm_inp = concatenate([inp_w2v,emb], axis = 2, name = 'lstm_inp')\n",
    "    \n",
    "    lstm_1 = Bidirectional(LSTM(500,name = 'lstm_layer'))(lstm_inp)\n",
    "    hidden_1 = Dropout(0.2, name = 'hidden_2')(lstm_1)\n",
    "    \n",
    "    output = Dense(e[0].shape[0],activation='linear',name = 'output')(hidden_1)\n",
    "    \n",
    "    distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([processed_a, processed_b]\n",
    "    \n",
    "    model = Model(inputs=[inp_context_words,inp_w2v],outputs = output)\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp_context_words (InputLayer)  (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb1 (Embedding)                (None, 10, 100)      2248300     inp_context_words[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inp_w2v (InputLayer)            (None, 10, 100)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb2 (Dropout)                  (None, 10, 100)      0           emb1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_inp (Concatenate)          (None, 10, 200)      0           inp_w2v[0][0]                    \n",
      "                                                                 emb2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1000)         2804000     lstm_inp[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hidden_2 (Dropout)              (None, 1000)         0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 250)          250250      hidden_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,302,550\n",
      "Trainable params: 5,302,550\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = \"adam\"\n",
    "loss = cosine_distance\n",
    "metrics = costine_distance\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "validation_split = 0.1\n",
    "verbose = 1\n",
    "\n",
    "model = create_model(optimizer,loss,metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12747 samples, validate on 1417 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "LSTM_history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_split=validation_split, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set accuracy\n",
    "y_test_emb = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
