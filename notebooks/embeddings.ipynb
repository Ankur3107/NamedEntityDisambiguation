{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import knowledge_graph_extraction\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Vec Graphical Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database\n",
    "db = sqlite3.connect('../data/kensho.db')\n",
    "c = db.cursor()\n",
    "\n",
    "# visualize query into pandas dataframe\n",
    "def viz_tables(cols, query):\n",
    "    q = c.execute(query).fetchall()\n",
    "    framelist = dict()\n",
    "    for i, col_name in enumerate(cols):\n",
    "        framelist[col_name] = [col[i] for col in q]\n",
    "    return pd.DataFrame.from_dict(framelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.execute(\"\"\"DROP TABLE IF EXISTS title_triplets\"\"\")\n",
    "\n",
    "# query = \"\"\"\n",
    "# CREATE TABLE \n",
    "#     title_triplets AS \n",
    "# SELECT w1.target_page_title, t.edge_property_id, w2.target_page_title\n",
    "# FROM wikipage_triplets t LEFT JOIN wikipages_cleaned as w1\n",
    "# ON t.source_item_id = w1.wikidata_numeric_id\n",
    "# LEFT JOIN wikipages_cleaned as w2\n",
    "# ON t.target_item_id = w2.wikidata_numeric_id\n",
    "# \"\"\"\n",
    "# c.execute(query)\n",
    "# db.commit()\n",
    "\n",
    "# query = \"\"\"\n",
    "# SELECT *\n",
    "# FROM title_triplets t\n",
    "# LIMIT 1000\n",
    "# \"\"\"\n",
    "# triplets = viz_tables(['source_title', 'edge_property', 'target_title'], query)\n",
    "# triplets.head()\n",
    "\n",
    "# db = '../data/kensho.db'\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT source_item_id, edge_property_id, target_item_id\n",
    "FROM wikipage_triplets\n",
    "\"\"\"\n",
    "\n",
    "triplets = knowledge_graph_extraction.conduct_sql_query(db, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets[['source_item_id', 'target_item_id']].to_csv('../data/sample_graph.edgelist', \n",
    "                                                     header=None,\n",
    "                                                     index=False,\n",
    "                                                     sep=' ',\n",
    "                                                     mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import node2vec\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def read_graph(data, weighted=False, directed=False):\n",
    "    '''\n",
    "    Reads the input network in networkx.\n",
    "    '''\n",
    "    if weighted:\n",
    "        G = nx.read_edgelist(data, nodetype=int, data=(('weight',float),), create_using=nx.DiGraph())\n",
    "    else:\n",
    "        G = nx.read_edgelist(data, nodetype=int, create_using=nx.DiGraph())\n",
    "        for edge in G.edges():\n",
    "            G[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "    if not directed:\n",
    "        G = G.to_undirected()\n",
    "\n",
    "    return G\n",
    "\n",
    "def learn_embeddings(walks, size=128, window_size=10, min_count=0, workers=1, epochs=1):\n",
    "    '''\n",
    "    Learn embeddings by optimizing the Skipgram objective using SGD.\n",
    "    '''\n",
    "    walks = [list(map(str, walk)) for walk in walks]\n",
    "    model = Word2Vec(walks, \n",
    "                     size=size, # number of dimensions\n",
    "                     window=window_size, # context size for optimization\n",
    "                     min_count=min_count, \n",
    "                     sg=1, \n",
    "                     workers=workers, # number of parallel workers\n",
    "                     iter=epochs) # number of epochs in SGD\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/node2vec-embeddings-for-graph-data-32a866340fef\n",
    "# not the author, but gives good introduction to node2vec\n",
    "# read input network in networkx\n",
    "nx_G = read_graph('../data/sample_graph.edgelist')\n",
    "G = node2vec.Graph(nx_G, # network graph\n",
    "                   False, # directed or not\n",
    "                   1, # return hyperparameter: probability of returning to previous node\n",
    "                   1) # in-out hyperparameter: probability of exploring undiscovered parts of graph\n",
    "G.preprocess_transition_probs()\n",
    "walks = G.simulate_walks(10, # num of walks: how many random walks for each node in graph \n",
    "                         80) # walk length: how many nodes in each random walk\n",
    "model = learn_embeddings(walks, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_id = '31'\n",
    "print('Page Title: {}'.format(id_title(wikidata, page_id)))\n",
    "for i in model.wv.most_similar(page_id):\n",
    "    print('Similar to {}, with similarity score {}'\n",
    "          .format(id_title(wikidata, i[0]), i[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../data/graph_embedding.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently just using Word2Vec on entities, instead of full Wikipedia text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all entities\n",
    "import re\n",
    "all_entities = []\n",
    "for text in data:\n",
    "    text_entities = []\n",
    "    for key, _ in text[1].items():\n",
    "        # get phrases underscored for phrase2vec\n",
    "        entity = key[0].replace(' ', '_')\n",
    "        text_entities.append(entity)\n",
    "    all_entities.append(text_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using only a sequence of entities and not full sentences for now\n",
    "entity_model = Word2Vec(all_entities, size=128, window=10, min_count=1, workers=8, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_model.save('../data/entity_embedding.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
