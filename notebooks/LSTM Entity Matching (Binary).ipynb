{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Input, Concatenate, Dropout, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import backend as K\n",
    "\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intros\n",
    "\n",
    "with open('../data/sample_labels.pkl', 'rb') as f:\n",
    "    intros = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entity embeddings for each entity ID\n",
    "\n",
    "with open('../data/knowledge_graph_data/wiki_DistMult_entity.npy', 'rb') as f:\n",
    "    e = np.load(f)\n",
    "\n",
    "kg_emb_size = e[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entity id to embedding mapping. This is so we can retrieve the \n",
    "#entity embeddings when we know the index of the entity\n",
    "\n",
    "with open('../data/knowledge_graph_data/idx2id_entity_full_no_text.pickle', 'rb') as f:\n",
    "    idx2id = pickle.load(f)\n",
    "id2idx = {v: k for k, v in idx2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break intros into lists of the text, entity locations, and entity IDs\n",
    "\n",
    "num_entities = 0\n",
    "full_text = []\n",
    "entity_locations = []\n",
    "entity_id = []\n",
    "\n",
    "for intro in intros:\n",
    "    if intro[1]:\n",
    "        full_text.append(intro[0])\n",
    "        \n",
    "        temp = []\n",
    "        temp1 = []\n",
    "        \n",
    "        for idx,entity_key in enumerate(intro[1]):\n",
    "\n",
    "            temp.append(entity_key[3])\n",
    "            loc = np.argwhere(intro[1][entity_key]==1)\n",
    "            temp1.append((loc.min(),loc.max()))\n",
    "            num_entities+=1\n",
    "                \n",
    "        entity_id.append(temp)\n",
    "        entity_locations.append(temp1)\n",
    "\n",
    "full_text = np.asarray(full_text)\n",
    "entity_locations = np.asarray(entity_locations)\n",
    "entity_id = np.asarray(entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create paragraphs array.\n",
    "#The paragraph array is a list of sublists. Each sublist is a list of words contained in the paragraph.\n",
    "\n",
    "\n",
    "paragraphs = []\n",
    "for paragraph in full_text:\n",
    "    temp = []\n",
    "    for sentence in sent_tokenize(paragraph):\n",
    "        for word in word_tokenize(sentence):\n",
    "            temp.append(word)\n",
    "    paragraphs.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#train w2v model and create intro array\n",
    "w2v_size = 100\n",
    "w2v_model = Word2Vec(paragraphs, min_count = 1, size = w2v_size, window = 5, sg=1)\n",
    "vocab_size = len(w2v_model.wv.vocab)\n",
    "\n",
    "intro_vectors = []\n",
    "for sentence in paragraphs:\n",
    "    temp = []\n",
    "    for word in sentence:\n",
    "        temp.append(w2v_model[word])\n",
    "    intro_vectors.append(temp)\n",
    "\n",
    "intro_vectors = [[l.tolist() for l in vectors] for vectors in intro_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent success: 88.41346378914845\n"
     ]
    }
   ],
   "source": [
    "#get training data to be used for LSTM\n",
    "#X will be a list of sublists. Each sublist contains the vectors of words in the context window for each entity.\n",
    "\n",
    "context_window = 10 #number of words with entity centered for input to LSTM model\n",
    "\n",
    "#using text sequences\n",
    "X_words = []\n",
    "X_w2v = []\n",
    "X_comparisons = []\n",
    "count_fail=0\n",
    "count_success=0\n",
    "\n",
    "for idx,locations in enumerate(entity_locations):\n",
    "    for idx2,loc in enumerate(locations):\n",
    "        low = max(loc[0]-context_window//2,0)\n",
    "        r_extra = max(0,context_window//2-loc[0])\n",
    "        high = min(loc[0]+context_window//2,len(paragraphs[idx]))\n",
    "        l_extra = max(loc[0]+context_window//2-len(paragraphs[idx]),0)\n",
    "        try:\n",
    "            X_comparisons.append(e[id2idx[entity_id[idx][idx2]]])\n",
    "            X_words.append(paragraphs[idx][low-l_extra:high+r_extra])\n",
    "            X_w2v.append(intro_vectors[idx][low-l_extra:high+r_extra])\n",
    "            count_success+=1\n",
    "        except:\n",
    "            count_fail+=1\n",
    "print('Percent success: {}'.format(100*(count_success/(count_success+count_fail))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We cannot feed words into the LSTM. So we need to tokenize the words\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_words)\n",
    "X_token_words = np.zeros((len(X_words),context_window), dtype=int)\n",
    "for idx,window in enumerate(X_words):\n",
    "    for idx2,word in enumerate(window):\n",
    "        X_token_words[idx][idx2] = t.word_counts[word]\n",
    "num_unique_words = X_token_words.max()+1\n",
    "\n",
    "#Convert X_w2v list into array\n",
    "X_w2v_new = np.zeros((len(X_w2v),context_window,w2v_size))\n",
    "for idx,window in enumerate(X_w2v):\n",
    "    for idx2,word in enumerate(window):\n",
    "        for idx3,emb in enumerate(word):\n",
    "            X_w2v_new[idx][idx2][idx3] = emb\n",
    "X_w2v = X_w2v_new\n",
    "\n",
    "#convert comparisons into array\n",
    "X_comparisons = np.array(X_comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have correct training labels, let's re-organize our training data such that the problem can be posed as a binary classification problem, rather than forcing the model to learn the actual KG embeddings. For each example that we currently have (positive labels), let's give the model a few negative examples. We want negative examples to be KG embeddings that are relatively close to the positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_incorrect = 10\n",
    "    \n",
    "def get_X_incorrect():\n",
    "\n",
    "    closest_indices = np.zeros((len(Y),num_incorrect), dtype = int)\n",
    "\n",
    "    for idx,y in enumerate(Y):\n",
    "        distances = dict()\n",
    "        y_size = np.dot(y,y)\n",
    "        for idx1,emb in enumerate(Y):\n",
    "            if idx1 != idx:\n",
    "                distances[np.dot(y,emb)/(y_size*np.dot(emb,emb))] = idx1\n",
    "        for idx2,val in enumerate(sorted(distances)[-num_incorrect:]):\n",
    "            closest_indices[idx][idx2] = distances[val]\n",
    "\n",
    "    X_incorrect = []\n",
    "    for idx, entity in enumerate(closest_indices):\n",
    "        for idx1,incorrect_entity in enumerate(entity):\n",
    "            X_incorrect.append(e[incorrect_entity])\n",
    "    X_incorrect = np.array(X_incorrect)\n",
    "\n",
    "    np.save('X_incorrect.npy',X_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comparisons= np.append(X_comparisons,np.load('X_incorrect.npy'),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_all = []\n",
    "X_token_all = []\n",
    "for w2v_window,token_window in zip(X_w2v,X_token_words):\n",
    "    for i in range(num_incorrect):\n",
    "        X_w2v_all.append(w2v_window)\n",
    "        X_token_all.append(token_window)\n",
    "        \n",
    "X_w2v_all=np.append(X_w2v,np.array(X_w2v_all),axis = 0)\n",
    "X_token_all = np.append(X_token_words,np.array(X_token_all),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros(X_comparisons.shape[0])\n",
    "for idx,y in enumerate(Y):\n",
    "    if idx < X_w2v.shape[0]:\n",
    "        Y[idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words,X_test_words,X_train_w2v,X_test_w2v,X_train_comparisons,X_test_comparisons,Y_train,Y_test = train_test_split(\n",
    "    X_token_all,X_w2v_all,X_comparisons,Y)\n",
    "X_train = [X_train_words,X_train_w2v,X_train_comparisons]\n",
    "X_test = [X_test_words,X_test_w2v,X_test_comparisons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom loss function for cosine distance (for binary classification)\n",
    "\n",
    "def cosine_distance(vals):\n",
    "    ytrue = vals[0]\n",
    "    ypred = vals[1]\n",
    "    return -K.mean(ytrue * ypred, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer,loss,metrics):\n",
    "    \n",
    "    #inputs\n",
    "    inp_context_words = Input(shape = (context_window,), name='inp_context_words')\n",
    "    inp_w2v = Input(shape = (context_window,w2v_size), name = 'inp_w2v')\n",
    "    inp_comparisons = Input(shape = kg_emb_size,name = 'inp_comparisons')\n",
    "    \n",
    "    #embed the context words\n",
    "    emb = Embedding(output_dim = 100, input_dim = num_unique_words, input_length = context_window,\n",
    "                   name = 'emb1')(inp_context_words)\n",
    "    emb = Dropout(0.2, name = 'emb2')(emb)\n",
    "    \n",
    "    #LSTM input\n",
    "    lstm_inp = concatenate([inp_w2v,emb], axis = 2, name = 'lstm_inp')\n",
    "    \n",
    "    lstm_1 = Bidirectional(LSTM(500,name = 'lstm_layer'))(lstm_inp)\n",
    "    hidden_1 = Dropout(0.2, name = 'hidden_2')(lstm_1)\n",
    "    \n",
    "    lstm_output = Dense(e[0].shape[0],activation='linear',name = 'lstm_output')(hidden_1)\n",
    "    \n",
    "    distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([lstm_output, inp_comparisons])\n",
    "    \n",
    "    model = Model(inputs=[inp_context_words,inp_w2v,inp_comparisons],outputs = distance)\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1120 18:22:46.366942 139836567152448 deprecation_wrapper.py:119] From /home/matteo/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1120 18:22:46.379953 139836567152448 deprecation_wrapper.py:119] From /home/matteo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1120 18:22:46.383372 139836567152448 deprecation.py:323] From /home/matteo/.conda/envs/capstone/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp_context_words (InputLayer)  (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb1 (Embedding)                (None, 10, 100)      2248300     inp_context_words[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inp_w2v (InputLayer)            (None, 10, 100)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb2 (Dropout)                  (None, 10, 100)      0           emb1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_inp (Concatenate)          (None, 10, 200)      0           inp_w2v[0][0]                    \n",
      "                                                                 emb2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000)         2804000     lstm_inp[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hidden_2 (Dropout)              (None, 1000)         0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_output (Dense)             (None, 250)          250250      hidden_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "inp_comparisons (InputLayer)    (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           lstm_output[0][0]                \n",
      "                                                                 inp_comparisons[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 5,302,550\n",
      "Trainable params: 5,302,550\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = \"adam\"\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "validation_split = 0.1\n",
    "verbose = 1\n",
    "\n",
    "model = create_model(optimizer,loss,metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140228 samples, validate on 15581 samples\n",
      "Epoch 1/10\n",
      "140228/140228 [==============================] - 232s 2ms/step - loss: 0.3227 - acc: 0.9084 - val_loss: 0.2992 - val_acc: 0.9089\n",
      "Epoch 2/10\n",
      "140228/140228 [==============================] - 230s 2ms/step - loss: 0.2851 - acc: 0.9084 - val_loss: 0.2842 - val_acc: 0.9089\n",
      "Epoch 3/10\n",
      "140228/140228 [==============================] - 229s 2ms/step - loss: 0.2731 - acc: 0.9085 - val_loss: 0.2625 - val_acc: 0.9089\n",
      "Epoch 4/10\n",
      "140228/140228 [==============================] - 235s 2ms/step - loss: 0.2720 - acc: 0.9085 - val_loss: 0.2672 - val_acc: 0.9089\n",
      "Epoch 5/10\n",
      "140228/140228 [==============================] - 236s 2ms/step - loss: 0.2785 - acc: 0.9084 - val_loss: 0.2690 - val_acc: 0.9088\n",
      "Epoch 6/10\n",
      "140228/140228 [==============================] - 236s 2ms/step - loss: 0.2772 - acc: 0.9084 - val_loss: 0.2633 - val_acc: 0.9089\n",
      "Epoch 7/10\n",
      "140228/140228 [==============================] - 236s 2ms/step - loss: 0.2761 - acc: 0.9084 - val_loss: 0.2698 - val_acc: 0.9089\n",
      "Epoch 8/10\n",
      "140228/140228 [==============================] - 236s 2ms/step - loss: 0.2788 - acc: 0.9084 - val_loss: 0.2635 - val_acc: 0.9089\n",
      "Epoch 9/10\n",
      "140228/140228 [==============================] - 236s 2ms/step - loss: 0.2798 - acc: 0.9085 - val_loss: 0.2713 - val_acc: 0.9089\n",
      "Epoch 10/10\n",
      "140228/140228 [==============================] - 236s 2ms/step - loss: 0.2888 - acc: 0.9085 - val_loss: 0.2722 - val_acc: 0.9089\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "LSTM_history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_split=validation_split, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set accuracy\n",
    "y_test_emb = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
